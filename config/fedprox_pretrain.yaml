gpu: [0]
num_clients: 10
fraction: 1.0
num_rounds: 200
seed: 42
framework: "fedprox" # fedavg / fedpub / fedlw / hyperfedlw
eval_global: false
trial: test
# Data config
task: "cora_disjoint_0.2" # Define it in main -> data # cora / citeseer / pubmed
data_header: "heterogeneous_partition"

# Server config
l1: 0.001
mask_aggr: false # aggregate mask globally
aggregate_classifier: true
# Client config
loc_l2: 0.001
# Learning Config
optimizer: "adam"
optim_param: #optimizer parameters, If lr is not defined, use base_lr
  weight_decay: 0.000001
base_lr: 0.01
num_epochs: 3
loss_func: "cross_entropy"
loss_param:

# Model config
model: "maskedgcn" # gcn, maskedgcn, lwmaskedgcn
maskedgcn:
  clsf_mask_one: true
  laye_mask_one: true
  mask_drop: false 
  mask_drop_ratio: 0.5 
  mask_noise: false
use_mask: true
use_dropout: false
mask_rank: -1 
use_classifier_mask: false
num_dims: 128
num_layers: 2

# Path config
path_suffix: "_path"
base_path: "output"
checkpoint_path: "save/checkpoint/"
log_path: "save/log/"
data_path: "dataset/"

log_file: "log.log"
# Verbose config
verbose_print_setup: true
verbose_print_aggr: false
verbose_print_server: true
verbose_print_client: true
verbose_log_setup: true
verbose_log_aggr: true
verbose_log_server: true
verbose_log_client: true
# Mode config
debug: false
debug_config:
  num_clients: 10
  server_model_path: "history/"
  client_model_path: "history/"
  server_model_header: "cora_server"
  client_model_header: "cora_client"
  order_by: "worker_id"
  model_file_extension: pt
datetoken: true
tuning_analysis: true
tuning_config:
  # defined in main.py
  analyze_result_path: "tuning/"
  analyze_result_file: "_"
pretrain: True
pretrain_config: 
  pretrained_model_path: "pretrained/"
  server_pretrained_header: "fedprox_cora_server"
  client_pretrained_header: "fedprox_cora_client"
  order_by: "client_id"
  pretrained_model_file_extension: "pt"